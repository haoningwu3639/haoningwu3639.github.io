<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Haoning Wu</title>

  <meta name="author" content="Haoning Wu">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="data:image/svg+xml, <svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üßê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:1%;width:70%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Haoning Wu „ÄåÂê¥Êµ©ÂÆÅ„Äç</name>
                  </p>

                  <p>
                    I am currently a 4th-year PhD candidate at <a href="https://sjtu.edu.cn/">Shanghai Jiao Tong University (SJTU)</a>, fortunately advised by <a href="https://weidixie.github.io/index.html">Prof. Weidi Xie</a> and <a href="https://annzhanglion.github.io/">Prof. Ya Zhang</a>.
                    Before that, I received my B.S. degree in EE (<strong>IEEE Pilot Class</strong>) also from <a href="https://sjtu.edu.cn/">SJTU</a> in 2022.</p>
                  
                  <p>
                    I am generally interested in <strong>multi-modal learning</strong>, especially <strong>generative models</strong>, <strong>spatial intelligence</strong>, and AI4Sports.
                    My ultimate goal is to build an artificial general intelligence that surpasses humans in <strong>perception</strong>, <strong>thinking</strong>, and <strong>practical</strong> abilities.
                  </p>

                  <p>
                    <font color="red"> I am always eager to communicate and cooperate, so feel free to contact me!!! </font>
                  </p>

                  <p>
                    By the way, I am currently open to <strong>research internship</strong> opportunities related to <strong>multi-modal generation and understanding</strong>. 
                    Feel free to connect via email or WeChat.
                  </p>

                  <p>
                    Email: haoningwu3639 at gmail.com  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  WeChat: haoningwu_ 
                  </p>

                  <p style="text-align:center">
                    <a href="mailto:haoningwu3639@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="data/HaoningWu_CV.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=ia4M9mMAAAAJ">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/haoningwu3639/">Github</a> &nbsp/&nbsp
                    <a href="https://www.zhihu.com/people/wu-hao-zhu-34">Zhihu</a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/haoningwu0815/">LinkedIn</a>
                  </p>
                </td>
                <td style="padding:1%;width:30%;max-width:30%">
                  <a href="images/HaoningWu.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/HaoningWu.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <ul>
                    <li> [2025.11] One paper (<strong><a href="https://mengmouxu.github.io/SceneGen//">SceneGen</a></strong>) has been accepted by <strong><a href="https://3dvconf.github.io/2026/">3DV 2026</a></strong>. </li>
                    <li> [2025.08] Invited <a href="https://www.bilibili.com/video/BV19EeyzmENK/?vd_source=6cce745af23238982ade9525fe577332/">talk</a> about our paper (<strong><a href="https://haoningwu3639.github.io/MRGen/">MRGen</a></strong>) at <strong><a href="https://nice-nlp.github.io/">NICE</a></strong>. Welcome to discuss! </li>
                    <li> [2025.08] I will attend <strong><a href="https://iccv.thecvf.com/Conferences/2025">ICCV 2025</a></strong> in person. See you in Hawaii! </li>
                    <li> [2025.07] One paper (<strong><a href="https://jyrao.github.io/SoccerAgent/">SoccerAgent</a></strong>) has been accepted by <strong><a href="https://acmmm2025.org/">ACM Multimedia 2025</a></strong>. </li>
                    <li> [2025.06] One paper (<strong><a href="https://haoningwu3639.github.io/MRGen/">MRGen</a></strong>) has been accepted by <strong><a href="https://iccv.thecvf.com/Conferences/2025/">ICCV 2025</a></strong>. </li>
                    <li> [2025.02] One paper (<strong><a href="https://jyrao.github.io/UniSoccer/">UniSoccer</a></strong>) has been accepted by <strong><a href="https://cvpr.thecvf.com/Conferences/2025/">CVPR 2025</a></strong>. </li>
                    <li> [2024.11] I am recognized as <strong><a href="https://bmvc2024.org/people/reviewers/">BMVC 2024 Outstanding Reviewer</a></strong>. </li>
                    <li> [2024.10] One paper (<strong><a href="https://haoningwu3639.github.io/MegaFusion/">MegaFusion</a></strong>) has been accepted by <strong><a href="https://wacv2025.thecvf.com/">WACV 2025</a></strong>. </li>
                    <li> [2024.09] One paper (<strong><a href="https://haoningwu3639.github.io/MatchTime/">MatchTime</a></strong>) has been accepted by <strong><a href="https://2024.emnlp.org/">EMNLP 2024</a></strong> and selected as <strong>Oral</strong>. </li>
                    <!-- <li> [2024.06] Codes of <strong><a href="https://github.com/haoningwu3639/SimpleSDM-3">SimpleSDM-3</a></strong> and <a href="https://github.com/haoningwu3639/SimpleSDXL">SimpleSDXL</a> have been made public for helping research about Diffusion. </li> -->
                    <li> [2024.04] I will attend <strong><a href="https://cvpr.thecvf.com/Conferences/2024">CVPR 2024</a></strong> in person. See you in Seattle! </li>
                    <li> [2024.04] I have passed my PhD Qualification Examination, and I am a PhD candidate now. </li>
                    <li> [2024.03] Start internship at <strong><a href="https://www.shlab.org.cn/">Shanghai AI Laboratory</a></strong> as a research intern, Shanghai (on-site). </li>
                    <li> [2024.02] One paper (<strong><a href="https://haoningwu3639.github.io/StoryGen_Webpage/">StoryGen</a></strong>) has been accepted by <strong><a href="https://cvpr.thecvf.com/Conferences/2024">CVPR 2024</a></strong>. </li>
                    <!-- <li> [2023.12] Codes of <a href="https://github.com/haoningwu3639/SimpleSDM">SimpleSDM</a> and <a href="https://github.com/haoningwu3639/SimpleSDM-Video">SimpleSDM-Video</a> have been made public for helping research about Diffusion. </li> -->
                    <li> [2023.10] One paper (<a href="https://dl.acm.org/doi/10.1145/3595916.3626380">NeRF-SDP</a>) has been accepted by <a href="https://www.mmasia2023.org/">ACM Multimedia Asia 2023</a> and selected as <strong>Oral</strong>. </li>
                    <li> [2023.08] One paper (<strong><a href="https://haoningwu3639.github.io/VFI_Adapter_Webpage/">VFIAdapter</a></strong>) has been accepted by <strong><a href="https://bmvc2023.org/">BMVC 2023</a></strong> and selected as <strong>Oral</strong>. </li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>Preprints</heading>
                  <p>
                    * denotes equal contribution, and <sup>‚Ä†</sup> denotes corresponding author.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/SoccerMaster.png" alt="SoccerMaster" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://haolinyang-hlyang.github.io/SoccerMaster/">
                    <papertitle>SoccerMaster: A Vision Foundation Model for Soccer Understanding</papertitle>
                  </a>
                  <br>
                  <a href="https://haolinyang-hlyang.github.io/">Haolin Yang</a>, <a href="https://jyrao.github.io">Jiayuan Rao</a>, <strong>Haoning Wu</strong>, <a href="https://weidixie.github.io/">Weidi Xie<sup>‚Ä†</sup></a>
                  <br>
                  <em>ACM Multimedia</em>, 2025. &nbsp; <font color="red"> <strong>(NEW)</strong></font>
                  <br>
                  <a href="https://haolinyang-hlyang.github.io/SoccerMaster/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2512.11016/">arXiv</a>
                  /
                  <a href="https://github.com/haolinyang-hlyang/SoccerMaster/">code</a>
                  <br>
                  <p> In this work, we present SoccerMaster, the first soccer-specific vision foundation model that unifies diverse understanding tasks within a single framework.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="SpatialScore/resources/dataset.png" alt="SpatialScore" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://haoningwu3639.github.io/SpatialScore/">
                    <papertitle>SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding</papertitle>
                  </a>
                  <br>
                  <strong>Haoning Wu*</strong>, <a href="https://github.com/1106280506Hx/">Xiao Huang*</a>, <a href="https://github.com/YHChen0511/">Yaohui Chen</a>, <a href="https://annzhanglion.github.io/">Ya Zhang</a>, <a href="https://cmic.sjtu.edu.cn/wangyanfeng/">Yanfeng Wang<sup>‚Ä†</sup></a>, <a href="https://weidixie.github.io/">Weidi Xie<sup>‚Ä†</sup></a>
                  <br>
                  <em>arXiv</em>, 2025. &nbsp; <font color="red"> <strong>(NEW)</strong></font>
                  <br>
                  <a href="https://haoningwu3639.github.io/SpatialScore/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2505.17012/">arXiv</a>
                  /
                  <a href="https://github.com/haoningwu3639/SpatialScore/">code</a>
                  <br>
                  <p> In this work, we investigate a critical question: <i> to what extent do existing MLLMs possess spatial intelligence, encompassing both spatial perception and spatial understanding? </i>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>Publications</heading>
                  <p>
                    * denotes equal contribution, and <sup>‚Ä†</sup> denotes corresponding author.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/SceneGen.png" alt="SceneGen" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://mengmouxu.github.io/SceneGen/">
                    <papertitle>SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass</papertitle>
                  </a>
                  <br>
                  <a href="https://github.com/Mengmouxu/">Yanxu Meng*</a>, <strong>Haoning Wu*</strong>, <a href="https://annzhanglion.github.io/">Ya Zhang</a>, <a href="https://weidixie.github.io/">Weidi Xie<sup>‚Ä†</sup></a>
                  <br>
                  <em>3DV</em>, 2026. &nbsp; <font color="red"> <strong>(NEW)</strong></font>
                  <br>
                  <a href="https://mengmouxu.github.io/SceneGen/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2508.15769/">arXiv</a>
                  /
                  <a href="https://github.com/Mengmouxu/SceneGen/">code</a>
                  <br>
                  <p> In this work, we propose a feedforward 3D scene generation model that can simultaneously synthesize multiple 3D assets from a single image.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/SoccerAgent.png" alt="SoccerAgent" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://jyrao.github.io/SoccerAgent/">
                    <papertitle>Multi-Agent System for Comprehensive Soccer Understanding</papertitle>
                  </a>
                  <br>
                  <a href="https://jyrao.github.io">Jiayuan Rao*</a>, <a href="https://github.com/lizifeng1209">Zifeng Li*</a>, <strong>Haoning Wu</strong>, <a href="https://annzhanglion.github.io/">Ya Zhang</a>, <a href="https://cmic.sjtu.edu.cn/wangyanfeng/">Yanfeng Wang<sup>‚Ä†</sup></a>, <a href="https://weidixie.github.io/">Weidi Xie<sup>‚Ä†</sup></a>
                  <br>
                  <em>ACM Multimedia</em>, 2025. &nbsp; <font color="red"> <strong>(NEW)</strong></font>
                  <br>
                  <a href="https://jyrao.github.io/SoccerAgent/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2505.03735/">arXiv</a>
                  /
                  <a href="https://github.com/jyrao/SoccerAgent/">code</a>
                  <br>
                  <p> In this work, we present SoccerBench, the largest and most comprehensive soccer-specific benchmark, along with a multi-agent system, SoccerAgent, for soccer understanding.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="MRGen/resources/teaser.png" alt="MRGen" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://haoningwu3639.github.io/MRGen/">
                    <papertitle>MRGen: Segmentation Data Engine for Underrepresented MRI Modalities</papertitle>
                  </a>
                  <br>
                  <strong>Haoning Wu*</strong>, <a href="https://zhaoziheng.github.io/">Ziheng Zhao*</a>, <a href="https://annzhanglion.github.io/">Ya Zhang</a>, <a href="https://cmic.sjtu.edu.cn/wangyanfeng/">Yanfeng Wang<sup>‚Ä†</sup></a>, <a href="https://weidixie.github.io/">Weidi Xie<sup>‚Ä†</sup></a>
                  <br>
                  <em>ICCV</em>, 2025. &nbsp; <font color="red"> <strong>(NEW)</strong></font>
                  <br>
                  <a href="https://haoningwu3639.github.io/MRGen/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2412.04106/">arXiv</a>
                  /
                  <a href="https://github.com/haoningwu3639/MRGen/">code</a>
                  <br>
                  <p> In this work, we establish a novel paradigm for generative models in medical applications: controllably synthesizing data for underrepresented modalities.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/UniSoccer.png" alt="UniSoccer" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://jyrao.github.io/UniSoccer/">
                    <papertitle>Towards Universal Soccer Video Understanding</papertitle>
                  </a>
                  <br>
                  <a href="https://jyrao.github.io">Jiayuan Rao*</a>, <strong>Haoning Wu*</strong>, <a href="https://scholar.google.nl/citations?user=0TvdOEcAAAAJ&hl=en/">Hao Jiang</a>, <a href="https://annzhanglion.github.io/">Ya Zhang</a>, <a href="https://cmic.sjtu.edu.cn/wangyanfeng/">Yanfeng Wang<sup>‚Ä†</sup></a>, <a href="https://weidixie.github.io/">Weidi Xie<sup>‚Ä†</sup></a>
                  <br>
                  <em>CVPR</em>, 2025. &nbsp; <font color="red"> <strong>(NEW)</strong></font>
                  <br>
                  <a href="https://jyrao.github.io/UniSoccer/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2412.01820/">arXiv</a>
                  /
                  <a href="https://github.com/jyrao/UniSoccer/">code</a>
                  <br>
                  <p> In this work, we present the first visual-language foundation model tailored for soccer video understanding, which can be applied various downstream tasks.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="MegaFusion/resources/teaser.png" alt="MegaFusion" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://haoningwu3639.github.io/MegaFusion/">
                    <papertitle>MegaFusion: Extend Diffusion Models towards Higher-resolution Image Generation without Further Tuning</papertitle>
                  </a>
                  <br>
                  <strong>Haoning Wu*</strong>, <a href="https://github.com/ShaochengShen">Shaocheng Shen*</a>, <a href="https://qianghu-huber.github.io/qianghuhomepage/">Qiang Hu</a>, <a href="https://mediabrain.sjtu.edu.cn/xiaoyun-zhang/">Xiaoyun Zhang<sup>‚Ä†</sup></a>, <a href="https://annzhanglion.github.io/">Ya Zhang</a>, <a href="https://cmic.sjtu.edu.cn/wangyanfeng/">Yanfeng Wang</a>
                  <br>
                  <em>WACV</em>, 2025.
                  <br>
                  <a href="https://haoningwu3639.github.io/MegaFusion/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2408.11001/">arXiv</a>
                  /
                  <a href="https://github.com/ShaochengShen/MegaFusion/">code</a>
                  <br>
                  <p> In this work, we propose a tuning-free strategy to extend the higher-resolution image generation capabilities of existing diffusion models. </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="MatchTime/resources/architecture.png" alt="MatchTime" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://haoningwu3639.github.io/MatchTime/">
                    <papertitle>MatchTime: Towards Automatic Soccer Game Commentary Generation</papertitle>
                  </a>
                  <br>
                  <a href="https://jyrao.github.io">Jiayuan Rao*</a>, <strong>Haoning Wu*</strong>, <a href="https://verg-avesta.github.io/">Chang Liu</a>, <a href="https://cmic.sjtu.edu.cn/wangyanfeng/">Yanfeng Wang<sup>‚Ä†</sup></a>, <a href="https://weidixie.github.io/">Weidi Xie<sup>‚Ä†</sup></a>
                  <br>
                  <em>EMNLP</em>, 2024. &nbsp; <font color="red"> <strong> (Oral Presentation)</strong></font>
                  <br>
                  <a href="https://haoningwu3639.github.io/MatchTime/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2406.18530/">arXiv</a>
                  /
                  <a href="https://github.com/jyrao/MatchTime/">code</a>
                  <br>
                  <p> In this work, we focus on building an visual-language model for automatic soccer game commentary generation. </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="StoryGen_Webpage/resources/teaser.png" alt="StoryGen" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://haoningwu3639.github.io/StoryGen_Webpage/">
                    <papertitle>Intelligent Grimm - Open-ended Visual Storytelling via Latent Diffusion Models</papertitle>
                  </a>
                  <br>
                  <a href="https://verg-avesta.github.io/">Chang Liu*</a>, <strong>Haoning Wu*</strong>, <a href="https://y-zhong.info/">Yujie Zhong</a>, <a href="https://mediabrain.sjtu.edu.cn/xiaoyun-zhang/">Xiaoyun Zhang</a>, <a href="https://cmic.sjtu.edu.cn/wangyanfeng/">Yanfeng Wang<sup>‚Ä†</sup></a>, <a href="https://weidixie.github.io/">Weidi Xie<sup>‚Ä†</sup></a>
                  <br>
                  <em>CVPR</em>, 2024.
                  <br>
                  <a href="https://haoningwu3639.github.io/StoryGen_Webpage/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2306.00973/">arXiv</a>
                  /
                  <a href="https://github.com/haoningwu3639/StoryGen/">code</a>
                  <br>
                  <p> In this work, we focus on the task of generating a series of coherent image sequence based on a given storyline, denoted as open-ended visual storytelling. </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/NeRFSDP.jpg" alt="nerf_sdp" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://dl.acm.org/doi/abs/10.1145/3595916.3626380/">
                    <papertitle>NeRF-SDP: Efficient Generalizable Neural Radiance Field with Scene Depth Perception</papertitle>
                  </a>
                  <br>
                  <a href="https://wangqiuwen1006.github.io/">Qiuwen Wang</a>, <a href="https://scholar.google.com/citations?user=ZMlpuqsAAAAJ&hl=zh-CN/">Shuai Guo</a>, <strong>Haoning Wu</strong>, <a href="https://medialab.sjtu.edu.cn/author/rong-xie/">Rong Xie</a>, <a href="https://medialab.sjtu.edu.cn/author/li-song/">Li Song<sup>‚Ä†</sup></a>, <a href="https://ee.sjtu.edu.cn/FacultyDetail.aspx?id=14&infoid=66&flag=66/">Wenjun Zhang</a>
                  <br>
                  <em>ACM Multimedia Asia</em>, 2023. &nbsp; <font color="red"> <strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://dl.acm.org/doi/abs/10.1145/3595916.3626380/">paper</a>
                  /
                  <a href="https://github.com/wangqiuwen1006/NeRF-SDP">code</a>
                  <br>
                  <p> In this work, we propose a novel framework, termed as NeRF-SDP, to address the challenge of balancing rendering speed and quality in generalizable NeRF.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="VFI_Adapter_Webpage/resources/teaser.png" alt="vfi_adapter" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://haoningwu3639.github.io/VFI_Adapter_Webpage/">
                    <papertitle>Boost Video Frame Interpolation via Simple Motion Adaptation</papertitle>
                  </a>
                  <br>
                  <strong>Haoning Wu</strong>, <a href="https://mediabrain.sjtu.edu.cn/xiaoyun-zhang/">Xiaoyun Zhang<sup>‚Ä†</sup></a>, <a href="https://weidixie.github.io/">Weidi Xie</a>, <a href="https://annzhanglion.github.io/">Ya Zhang</a>, <a href="https://cmic.sjtu.edu.cn/wangyanfeng/">Yanfeng Wang<sup>‚Ä†</sup></a>
                  <br>
                  <em>BMVC</em>, 2023. &nbsp; <font color="red"> <strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://haoningwu3639.github.io/VFI_Adapter_Webpage/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2306.13933/">arXiv</a>
                  /
                  <a href="https://github.com/haoningwu3639/VFI_Adapter">code</a>
                  <br>
                  <p> In this work, we propose a novel optimization-based VFI method that can adapt to unseen motions at test time and boost existing pre-trained models.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/LAR-SR.jpg" alt="lar_sr" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Guo_LAR-SR_A_Local_Autoregressive_Model_for_Image_Super-Resolution_CVPR_2022_paper.html?fbclid=IwAR0id_m5BNV2nYNcohdnF434oEPBMuOK6K6GQ4vQgEpa47BelUKN3HIAmZ4">
                    <papertitle>LAR-SR: A Local Autoregressive Model for Image Super-Resolution</papertitle>
                  </a>
                  <br>
                  <a href="https://github.com/guobaisong">Baisong Guo*</a>, <a href="https://mediabrain.sjtu.edu.cn/xiaoyun-zhang/">Xiaoyun Zhang*<sup>‚Ä†</sup></a>, <strong>Haoning Wu</strong>, <a href="https://mediabrain.sjtu.edu.cn/yuwang/">Yu Wang</a>, <a href="https://annzhanglion.github.io/">Ya Zhang</a>, <a href="https://cmic.sjtu.edu.cn/wangyanfeng/">Yanfeng Wang<sup>‚Ä†</sup></a>
                  <br>
                  <em>CVPR</em>, 2022.
                  <br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_LAR-SR_A_Local_Autoregressive_Model_for_Image_Super-Resolution_CVPR_2022_paper.pdf">paper</a>
                  /
                  <a href="https://github.com/haoningwu3639/LAR-SR/">code</a>
                  <br>
                  <p> In this work, we propose LAR-SR for super-resolution based on a Local AutoRegessive module, achieving superior performance among generative models for SR.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>Reviewer Service</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <ul>  
                    <li> Computer Vision and Pattern Recognition (<strong>CVPR</strong> <a href="https://cvpr.thecvf.com/Conferences/2023/">2023</a>, <a href="https://cvpr.thecvf.com/Conferences/2024/">2024</a>, <a href="https://cvpr.thecvf.com/Conferences/2025/">2025</a>, <a href="https://cvpr.thecvf.com/Conferences/2026/">2026</a>) </li>
                    <li> International Conference on Computer Vision (<strong>ICCV</strong> <a href="https://iccv2023.thecvf.com/">2023</a>, <a href="https://iccv2025.thecvf.com/">2025</a>) </li>
                    <li> European Conference on Computer Vision (<strong>ECCV</strong> <a href="https://eccv.ecva.net/Conferences/2024/">2024</a>) </li>
                    <li> ACM Multimedia (<strong>ACM MM</strong> <a href="https://2024.acmmm.org/">2024</a>, <a href="https://2025.acmmm.org/">2025</a>) </li>
                    <li> British Machine Vision Conference (<strong>BMVC</strong> <a href="https://bmvc2024.org/">2024</a>, <a href="https://bmvc2025.org/">2025</a>) (Outstanding Reviewer in <a href="https://bmvc2024.org/people/reviewers/">2024</a>) </li>
                    <li> AAAI Conference on Artificial Intelligence (<strong>AAAI</strong> <a href="https://aaai.org/conference/aaai/aaai-25/">2025</a>, <a href="https://aaai.org/conference/aaai/aaai-26/">2026</a>)
                    <li> Conference on Neural Information Processing Systems (<strong>NeurIPS</strong> <a href="https://neurips.cc/Conferences/2025">2025</a>) </li>
                    <!-- <li> ACL - Student Research Workshop (<strong>ACL-SRW</strong> <a href="https://2025.aclweb.org/calls/student_research_workshop/">2025</a>) </li> -->
                     <!-- <li> MICCAI - MLLMs in Clinical Practice Workshop (<strong>MICCAI-MLLMCP</strong> <a href="https://clinicalmllms.github.io/">2025</a>) </li> -->
                    <li> Winter Conference on Applications of Computer Vision (<strong>WACV</strong> <a href="https://wacv.thecvf.com/Conferences/2026/">2026</a>) </li>
                    <li> International Conference on 3D Vision (<strong>3DV</strong> <a href="https://3dvconf.github.io/2026/">2026</a>) </li>
                    
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>Awards</heading>
                </td>
              </tr>
            </tbody>
          </table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <!-- <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/SJTU_logo.png" style="width:80%;max-width:200px">
                </td> -->
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <ul>
                    <li> [2024] <a href="https://bmvc2024.org/people/reviewers/">BMVC 2024 Outstanding Reviewer</a> </li> 
                    <li> [2021] <strong>China National Scholarship</strong> (for Undergraduates) </li>
                    <li> [2021] School Scholarship B Prize </li>
                    <li> [2020] School Scholarship C Prize </li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=5ayBqtHVZxq64yIbqL91zvhcK44KOAMiyFcLXB3F2nc&cl=ffffff&w=a"></script>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <p style="text-align:left;font-size:small;"> Updated in December. 2025
                  </p>
                  <p style="text-align:right;font-size:small;">
                    Thanks <a href="https://jonbarron.info/"> Jon Barron</a> for this amazing website template</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
    </tbody>
  </table>
</body>

</html>