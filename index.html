<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Haoning Wu</title>
  
  <meta name="author" content="Haoning Wu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml, <svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üßê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Haoning Wu „ÄåÂê¥Êµ©ÂÆÅ„Äç</name>
              </p>
              <p>
                I am currently a 2nd-year master student of <a href="https://mediabrain.sjtu.edu.cn/">MediaBrain</a> at <a href="https://sjtu.edu.cn/">Shanghai Jiao Tong University (SJTU)</a>, advised by <a href="https://weidixie.github.io/index.html">Prof. Weidi Xie</a> and <a href="https://mediabrain.sjtu.edu.cn/xiaoyun-zhang/">Prof. Xiaoyun Zhang</a>.
                I received my Bachelor degree in EE (IEEE honor class) from <a href="https://sjtu.edu.cn/">Shanghai Jiao Tong University (SJTU)</a> in June 2022.</p>
              <p>
                I'm generally interested in computer vision and multimodal perception.
                Feel free to contact me via my email!!!
              </p>
              <p style="text-align:center">
                <a href="mailto:haoningwu3639@gmail.com">Email</a> &nbsp/&nbsp
                <!-- <a href="data/CV.pdf">CV</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=ia4M9mMAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/haoningwu3639/">Github</a> &nbsp/&nbsp
                <a href="https://www.zhihu.com/people/wu-hao-zhu-34">Zhihu</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/haoningwu0815/">LinkedIn</a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/HaoningWu.jpg"><img style="width:60%;max-width:60%" alt="profile photo" src="images/HaoningWu.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <heading>News</heading>
            <ul>
              <li> [2023.12] Codes of SimpleSDM and SimpleSDM-Video have been made public for helping research about Diffusion. </li>
              <li> [2023.10] I will attend BMVC 2023 and give an oral presentation in Aberdeen, UK this November. </li>
              <li> [2023.10] One paper (NeRF-SDP) has been accepted by ACM Multimedia Asia 2023 and selected as <strong>Oral</strong>. </li>
              <li> [2023.08] One paper (VFIAdapter) has been accepted by <strong>BMVC 2023</strong> and selected as <strong>Oral</strong>. </li>
              <li> [2023.07] Codes of VFIAdapter and StoryGen have been made public. </li>
            </ul>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <heading>Preprints</heading>
            <p>
              <sup>*</sup> denotes equal contribution, and
              <sup>‚Ä†</sup> denotes corresponding author.
            </p>
          </td>
        </tr>
      </tbody></table>

       <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/StoryGen.jpg" alt="storygen" width="200">
            </td>
            <td width="75%" valign="center">
              <a href="https://haoningwu3639.github.io/StoryGen_Webpage/">
                <papertitle>Intelligent Grimm - Open-ended Visual Storytelling via Latent Diffusion Models</papertitle>
              </a>
              <br>
            <a href="https://verg-avesta.github.io/">Chang Liu*</a>, <strong>Haoning Wu*</strong>, <a href="https://y-zhong.info/">Yujie Zhong</a>, <a href="https://mediabrain.sjtu.edu.cn/xiaoyun-zhang/">Xiaoyun Zhang<sup>‚Ä†</sup></a>, <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang<sup>‚Ä†</sup></a>, <a href="https://weidixie.github.io/">Weidi Xie<sup>‚Ä†</sup></a>
              <br>
              <em>arxiv Preprint</em>, 2023. &nbsp; <font color="red"> <strong>(NEW)</strong></font>
              <br>
              <a href="https://haoningwu3639.github.io/StoryGen_Webpage/">project page</a>
               / 
              <a href="https://arxiv.org/abs/2306.00973/">arXiv</a>
               / 
              <a href="https://github.com/haoningwu3639/StoryGen/">code</a>
              <br>
              <p> In this work, we focus on the task of generating a series of coherent image sequence based on a given storyline, denoted as open-ended visual storytelling. </p>
            </td>
          </tr>
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <heading>Publications</heading>
              <p>
                <sup>*</sup> denotes equal contribution, and
                <sup>‚Ä†</sup> denotes corresponding author.
              </p>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/NeRFSDP.jpg" alt="vfi_adapter" width="200">
            </td>
            <td width="75%" valign="center">
              <a href="https://github.com/wangqiuwen1006/NeRF-SDP">
                <papertitle>NeRF-SDP: Efficient Generalizable Neural Radiance Field with Scene Depth Perception</papertitle>
              </a>
              <br>
              <a href="https://wangqiuwen1006.github.io/">QiuWen Wang</a>, <a href="https://scholar.google.com/citations?user=ZMlpuqsAAAAJ&hl=zh-CN/">Shuai Guo</a>, <strong>Haoning Wu</strong>, <a href="https://medialab.sjtu.edu.cn/author/rong-xie/">Rong Xie</a>, <a href="https://medialab.sjtu.edu.cn/author/li-song/">Li Song<sup>‚Ä†</sup></a>, <a href="https://ee.sjtu.edu.cn/FacultyDetail.aspx?id=14&infoid=66&flag=66/">Wenjun Zhang</a>
              <br>
              <em>ACM Multimedia Asia</em>, 2023. &nbsp; <font color="red"> <strong>(Oral Presentation)</strong> &nbsp; <strong>(NEW)</strong></font>
              <br>
              <a href="https://arxiv.org/">arXiv</a> (soon)
              / 
              <a href="https://github.com/wangqiuwen1006/NeRF-SDP">code</a>
            <br>
              <p> In this paper, we propose a novel framework, termed as NeRF-SDP, to address the challenge of balancing rendering speed and quality in generalizable NeRF.</p>
            </td>
          </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/VFIAdapter.jpg" alt="vfi_adapter" width="200">
            </td>
            <td width="75%" valign="center">
              <a href="https://haoningwu3639.github.io/VFI_Adapter_Webpage/">
                <papertitle>Boost Video Frame Interpolation via Simple Motion Adaptation</papertitle>
              </a>
              <br>
              <strong>Haoning Wu</strong>, <a href="https://mediabrain.sjtu.edu.cn/xiaoyun-zhang/">Xiaoyun Zhang<sup>‚Ä†</sup></a>, <a href="https://weidixie.github.io/">Weidi Xie</a>, <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>, <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang<sup>‚Ä†</sup></a>
              <br>
              <em>BMVC</em>, 2023. &nbsp; <font color="red"> <strong>(Oral Presentation)</strong> &nbsp; <strong>(NEW)</strong></font>
              <br>
              <a href="https://haoningwu3639.github.io/VFI_Adapter_Webpage/">project page</a>
              / 
              <a href="https://arxiv.org/abs/2306.13933/">arXiv</a>
              / 
              <a href="https://github.com/haoningwu3639/VFI_Adapter">code</a>
            <br>
              <p> In this paper, we propose a novel optimization-based VFI method that can adapt to unseen motions at test time and boost existing pre-trained models.</p>
            </td>
          </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/LAR-SR.jpg" alt="lar_sr" width="200">
            </td>
            <td width="75%" valign="center">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Guo_LAR-SR_A_Local_Autoregressive_Model_for_Image_Super-Resolution_CVPR_2022_paper.html?fbclid=IwAR0id_m5BNV2nYNcohdnF434oEPBMuOK6K6GQ4vQgEpa47BelUKN3HIAmZ4">
                <papertitle>LAR-SR: A Local Autoregressive Model for Image Super-Resolution</papertitle>
              </a>
              <br>
              <a href="https://github.com/guobaisong">Baisong Guo*</a>, <a href="https://mediabrain.sjtu.edu.cn/xiaoyun-zhang/">Xiaoyun Zhang*<sup>‚Ä†</sup></a>, <strong>Haoning Wu</strong>, <a href="https://mediabrain.sjtu.edu.cn/yuwang/">Yu Wang</a>, <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>, <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang<sup>‚Ä†</sup></a>
              <br>
              <em>CVPR</em>, 2022.
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_LAR-SR_A_Local_Autoregressive_Model_for_Image_Super-Resolution_CVPR_2022_paper.pdf">paper</a>
              /
              <a href="https://github.com/haoningwu3639/LAR-SR/">code</a>
              <br>
              <p> We propose a novel approach called LAR-SR for super-resolution based on a Local AutoRegessive module, 
                    which achieves superior performance compared with other generative models for SR.</p>
            </td>
          </tr>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Reviewer Service</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              2023 Computer Vision and Pattern Recognition (<a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>)
              <br>
              2023 International Conference on Computer Vision (<a href="https://iccv2023.thecvf.com/">ICCV 2023</a>)
              <br>
              2024 Computer Vision and Pattern Recognition (<a href="https://cvpr2024.thecvf.com/">CVPR 2024</a>)
            </td>
          </tr>
          
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Awards</heading>
          </td>
        </tr>
      </tbody></table>
            <table width="100%" align="center" border="0" cellpadding="20"><tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/SJTU_logo.png" width="160"></td>
                <td width="75%" valign="center">
            <p>
              <strong>2020-2021 China National Scholarship</strong>
              <!-- <br> 2020-2021 ÂÖ®ÂõΩÂ§ßÂ≠¶ÁîüÁªøËâ≤ËÆ°ÁÆóÂàõÊñ∞Â§ßËµõÂõ¢Èòü‰∏ÄÁ≠âÂ•ñ -->
              <br> 2020-2021 School Scholarship B Prize
              <br> 2020-2021 Outstanding Student Leader of Shanghai Jiao Tong University
              <br> 2019-2020 School Scholarship C Prize
              <br> 2019-2020 Three Good Student of Shanghai Jiao Tong University
            </p>
          </td>
        </tr>
      </tbody></table>


      <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Work and Internship</heading>
        </td>
      </tr>
    </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/EE208.jpg" width="160"></td>
        <td width="75%" valign="center">
          T.A. of EE208: 'Introduction to Electrical Engineering C', Fall 2022. 
          <br> (instructors: Prof. <a href='https://mediabrain.sjtu.edu.cn/yazhang/'>Ya Zhang</a> & Prof. Dazhi He)
          <br>
          Cluster Server Administrator of <a href="https://mediabrain.sjtu.edu.cn/">MediaBrain@SJTU</a>, Spring 2023.
        </td>
      </tr>
      
    </tbody></table> -->

    <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=5ayBqtHVZxq64yIbqL91zvhcK44KOAMiyFcLXB3F2nc&cl=ffffff&w=a"></script>
    
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
            </p>
            <p style="text-align:left;font-size:small;"> Updated in December. 2023
            </p>
              <p style="text-align:right;font-size:small;">
                Thanks <a href="https://jonbarron.info/"> Jon Barron</a> for this amazing website template</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
